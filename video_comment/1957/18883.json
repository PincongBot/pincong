{"type":"video_comment","id":18883,"parentType":"video","parentId":1957,"uid":18160,"contents":"https://github.com/Rochester-NRT/RocAlphaGo/tree/develop/AlphaGo<br>\n<br>\ngo目錄，實現了env。對我來説，game_state.py 是我唯一感興趣的<br>\nmodels目錄，實現了3個神經網絡，重點啊！<br>\npreprocessing目錄，將數據格式化為tensor，重點！<br>\ntraining目錄，backward算法，重點！<br>\nai.py文件，實現了agent。隨便看看吧。。。<br>\nmcts.py文件，實現了mcts算法，重點！<br>\nutil.py文件，sgf 轉換，沒興趣。。。<br>\n<br>\n<br>\n<br>\n我的理解：<br>\nenv = Env::new()<br>\nagent = Agent::new()<br>\n<br>\nlet move = agent.get_move(env.current_state())<br>\nenv.do_move(move)<br>\nloop:<br>\n&nbsp;&nbsp;&nbsp;&nbsp; move = agent.get_move(env.current_state())<br>\n&nbsp;&nbsp;&nbsp;&nbsp; env.do_move(move)<br>\nend loop<br>\n<br>\nlet score = env.get_score()<br>\nmax!(score)<br>\n<br>\n<br>\nenv 就是這個圍棋 也就是 GameState<br>\nagent 就是player，自己實現算法來玩游戲<br>\nget_move 表示根據當前游戲局面，返回勝率最大的action<br>\n將 move 設置到 gamestate，更新 state<br>\n重複這個過程<br>\n得到state最後的結果，贏或者輸！<br>\n重複n次，希望贏的次數最多<br>\n<br>\n<br>\n<br>\n明天寫下train，也就是反向過程！<br>\n<br>\n<br>\n這個是比較原始的的AlphaGO<br>\n我自己看的是 AlphaGoZero ...","date":"2020-05-14","agreeCount":0,"discussionCount":0}