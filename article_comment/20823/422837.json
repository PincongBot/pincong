{"type":"article_comment","id":422837,"parentType":"article","parentId":20823,"uid":18160,"contents":"回到train上了<br>\n<br>\n# 损失函数<br>\n前面说的的，都是我们知道了f(x,y)这个曲线，求面积<br>\n<br>\n现在反过来，我们只知道面积，如何求f(x,y)呢？<br>\n<br>\n我们来学习拟合算法<br>\n<br>\n因为我们知道 每一层的结构都是 y = wx+b<br>\n所以求曲线的问题，等于我们知道 y 和x 的情况下，求 w和b的值<br>\n<br>\n如何去拟合呢 ？<br>\n保证 x带入拟合公式以后 sigma(y_real - y_predict)²的值最小<br>\n<br>\n也就是方差最小的时候，拟合度最高，这个方法叫MSE<br>\n用这个方法的损失函数叫MSELoss<br>\n<br>\n为什么叫损失函数呢？因为它表达了我们的拟合结果和真实结果的损失！（方差是干嘛的你们总知道吧）<br>\n我们希望损失尽可能小！（方差越小，表示拟合度越高）<br>\n<br>\n如果是x为向量的情况，则希望每个向量的拟合度尽可能高，也就是交叉熵<br>\n细说交叉熵，又来不及讲了。。。<br>\n<br>\n# 优化器<br>\n我们的目标是loss函数尽可能小<br>\n那么如何去加速知道w和b的值呢？<br>\n<br>\n很简单啊！<br>\nloss的最小值，肯定是 斜率为0 的时候，对不对？<br>\n<br>\n举个例子 ，y = x² <br>\n问，这个函数的最小值是多少？ 是不是斜率（导数）为0 的时候？<br>\n因为导数 &gt; 0 必然 向右的时候，y会变大<br>\n导数 &lt; 0 必然，向右的时候， y会变小<br>\n所以 导数 = 0 必然 是 极值点<br>\n<br>\n===<br>\n<br>\n为什么用 y = x²举例呢？因为 你把 mseloss 展开来看，就是一个 y = Kx² + balabala的一个函数。。。<br>\n<br>\n所以 ，当我们要求mseloss的最小值的时候，就会用导数【梯度】为0的特性来计算<br>\n如果梯度，大于0 则向左更新，否则向右更新，如果是0，则找到了极值点<br>\n<br>\n<br>\n这个方法叫梯度下降，自然有更好的梯度下降算法<br>\n比如随机梯度下降(sgd)<br>\n<br>\n除了sgd，还有Adam这类的明星优化器！<br>\n<br>\n日后展开继续吧<br>\n<br>\n<br>\n<br>\n<br>\n# 额外补充<br>\n过拟合问题<br>\n什么叫过拟合？就是井底之蛙的意思！<br>\n比如，某人在中国呆了一辈子，以为全人类都是说中文的，不知道外面还有人说英语，日语<br>\n再比如，某人觉得全人类吃饭都用刀叉，却不知道中国人用筷子，印度人用手抓。。。<br>\n<br>\n过拟合就是，train的时候，只考虑了某个子训练集，而在其他训练集的情况下表现很差<br>\n就是以样本 去 评估 全局，样本里表现虽然很好，但是全局表现很差<br>\n<br>\n解决办法，Dropout 或者 Batch Norm层<br>\n<br>\n---<br>\n梯度消失问题<br>\n因为网络深度加深了&nbsp; ，导致 0.x的 n 次方 ，无限逼近0 <br>\n怎么解决呢？就是深度残差网络了。。。<br>\n<br>\n下一期：<br>\n我再讲下自注意力机制<br>\n<br>\n<br>\ntransformer和BERT 还有他们的激活函数gelu","date":"2020-06-26","agreeCount":0,"discussionCount":0}